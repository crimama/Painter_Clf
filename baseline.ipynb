{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2 \n",
    "import pandas as pd \n",
    "from glob import glob \n",
    "from tqdm import tqdm \n",
    "import random \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "import cv2 \n",
    "\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.models as models \n",
    "\n",
    "from utils import img_dset\n",
    "from utils.Dataset import Data_loader,train_valid_split,Data_init\n",
    "from utils.Augmentation import valid_augmenter,train_augmenter,dacon_augmenter\n",
    "from utils.Trainer import epoch_run,valid_epoch_run\n",
    "from utils import Model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 48,  7, ..., 35, 35,  3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:32<00:00, 92.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0\n",
      "Train Loss : 3.462138407939188 | Train F1 : 0.06818589106273658\n",
      "Valid Loss : 2.8680980785472974 | Valid F1 : 0.12982510341519315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Fold : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m best \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(cfg\u001b[39m.\u001b[39mnum_epochs)):\n\u001b[0;32m---> 68\u001b[0m     epoch_loss,acc \u001b[39m=\u001b[39m epoch_run(train_loader,model,cfg\u001b[39m.\u001b[39;49mdevice,optimizer,criterion,scheduler,scaler)\n\u001b[1;32m     69\u001b[0m     epoch_loss_valid, acc_valid \u001b[39m=\u001b[39m valid_epoch_run(valid_loader,model,cfg\u001b[39m.\u001b[39mdevice,criterion)\n\u001b[1;32m     70\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m, epoch)\n",
      "File \u001b[0;32m/data/Painter_clf/utils/Trainer.py:14\u001b[0m, in \u001b[0;36mepoch_run\u001b[0;34m(data_loader, model, device, optimizer, criterion, scheduler, scaler)\u001b[0m\n\u001b[1;32m     12\u001b[0m epoch_target \u001b[39m=\u001b[39m [] \n\u001b[1;32m     13\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m batch_x,batch_labels \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     15\u001b[0m     batch_x \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     batch_labels \u001b[39m=\u001b[39m batch_labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/Painter_clf/utils/Dataset.py:24\u001b[0m, in \u001b[0;36mimg_dset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m,idx):\n\u001b[1;32m     22\u001b[0m     \u001b[39m#img load \u001b[39;00m\n\u001b[1;32m     23\u001b[0m     img_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_dirs[idx]\n\u001b[0;32m---> 24\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_load(img_dir)\n\u001b[1;32m     25\u001b[0m     \u001b[39m#resize \u001b[39;00m\n\u001b[1;32m     26\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugmenter(img)\n",
      "File \u001b[0;32m/data/Painter_clf/utils/Dataset.py:19\u001b[0m, in \u001b[0;36mimg_dset.img_load\u001b[0;34m(self, img_dir)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimg_load\u001b[39m(\u001b[39mself\u001b[39m,img_dir):\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m cv2\u001b[39m.\u001b[39;49mimread(img_dir)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class cfg:\n",
    "    img_size = 256\n",
    "    batch_size = 16\n",
    "    train_ratio = 0.8 \n",
    "    num_epochs = 30 \n",
    "    num_fold = 5\n",
    "    model_name = 'efficientnet_b0'\n",
    "    lr = 1e-4\n",
    "    device = 'cuda:0'\n",
    "    sava_path = './Save_models/'\n",
    "    seed = 41\n",
    "    crop_ratio = 0.5 \n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True   \n",
    "\n",
    "def model_save(model,path,model_name):\n",
    "    torch.save(model,path+f'save_models/{model_name}')    \n",
    "\n",
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"macro\")\n",
    "    return score\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "seed_everything(cfg.seed) # Seed 고정\n",
    "\n",
    "\n",
    "for k,(train_index,valid_index) in enumerate(KFold(n_splits=cfg.num_fold).split(img_dirs)):\n",
    "    img_dirs, labels = Data_init('./Data')\n",
    "    #데이터 로드 \n",
    "    if k == 0:\n",
    "        label_encoder = {value:key for key,value in enumerate(np.unique(labels))}\n",
    "    labels = pd.Series(labels).apply(lambda x : label_encoder[x]).values\n",
    "\n",
    "    #Train - Valid split \n",
    "        #train_img_dirs, valid_img_dirs = train_valid_split(img_dirs,cfg.train_ratio)\n",
    "        #train_labels, valid_labels = train_valid_split(labels,cfg.train_ratio)\n",
    "    train_img_dirs, valid_img_dirs = img_dirs[train_index],img_dirs[valid_index]\n",
    "    train_labels, valid_labels = labels[train_index],labels[valid_index]\n",
    "\n",
    "    #augmentation \n",
    "    Train_augmenter = dacon_augmenter(cfg)\n",
    "    Valid_augmenter = valid_augmenter(cfg) \n",
    "    \n",
    "    #데이터셋, 데이터 로더 \n",
    "    train_loader = Data_loader(train_img_dirs,train_labels,cfg,augmenter=Train_augmenter)\n",
    "    valid_loader = Data_loader(valid_img_dirs,valid_labels,cfg,augmenter=Valid_augmenter,shuffle=False)\n",
    "    \n",
    "    #모델 config \n",
    "    model = Model(cfg.model_name,num_classes=len(np.unique(labels))).to(cfg.device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(cfg.device)\n",
    "    optimizer = torch.optim.RAdam(model.parameters(),lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=100,eta_min=0)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    print(f'Current Fold : {k}')\n",
    "    best = 0\n",
    "    for epoch in tqdm(range(cfg.num_epochs)):\n",
    "        epoch_loss,acc = epoch_run(train_loader,model,cfg.device,optimizer,criterion,scheduler,scaler)\n",
    "        epoch_loss_valid, acc_valid = valid_epoch_run(valid_loader,model,cfg.device,criterion)\n",
    "        print('\\nEpoch ', epoch)\n",
    "        print(f'Train Loss : {epoch_loss} | Train F1 : {acc}')\n",
    "        print(f'Valid Loss : {epoch_loss_valid} | Valid F1 : {acc_valid}')\n",
    "\n",
    "        if epoch == 0:\n",
    "            best = acc_valid \n",
    "            torch.save(model,cfg.sava_path + f'{k}_fold_best.pt')\n",
    "        else:\n",
    "            if acc_valid > best:\n",
    "                torch.save(model,cfg.sava_path + f'{k}_fold_best.pt')\n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
