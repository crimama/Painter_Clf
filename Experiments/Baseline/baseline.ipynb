{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2 \n",
    "import pandas as pd \n",
    "from glob import glob \n",
    "from tqdm import tqdm \n",
    "import random \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from easydict import EasyDict as edict\n",
    "import json \n",
    "import cv2 \n",
    "import sys \n",
    "sys.path.append('../../')\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torchvision \n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.models as models \n",
    "\n",
    "from utils import img_dset\n",
    "from utils.Dataset import Data_loader,train_valid_split,Data_init\n",
    "from utils.Augmentation import valid_augmenter,train_augmenter,dacon_augmenter\n",
    "from utils.Trainer import epoch_run,valid_epoch_run\n",
    "from utils import Model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Fold : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s][ WARN:0@192.698] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('./Data/train/4223.jpg'): can't open/read file: check file path/integrity\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 64\u001b[0m\n\u001b[1;32m     62\u001b[0m best \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(cfg\u001b[39m.\u001b[39mnum_epochs)):\n\u001b[0;32m---> 64\u001b[0m     epoch_loss,acc \u001b[39m=\u001b[39m epoch_run(train_loader,model,cfg\u001b[39m.\u001b[39;49mdevice,optimizer,criterion,scheduler,scaler)\n\u001b[1;32m     65\u001b[0m     epoch_loss_valid, acc_valid \u001b[39m=\u001b[39m valid_epoch_run(valid_loader,model,cfg\u001b[39m.\u001b[39mdevice,criterion)\n\u001b[1;32m     66\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m'\u001b[39m, epoch)\n",
      "File \u001b[0;32m/data/Painter_clf/Experiments/Baseline/../../utils/Trainer.py:14\u001b[0m, in \u001b[0;36mepoch_run\u001b[0;34m(data_loader, model, device, optimizer, criterion, scheduler, scaler)\u001b[0m\n\u001b[1;32m     12\u001b[0m epoch_target \u001b[39m=\u001b[39m [] \n\u001b[1;32m     13\u001b[0m i \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \n\u001b[0;32m---> 14\u001b[0m \u001b[39mfor\u001b[39;00m batch_x,batch_labels \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     15\u001b[0m     batch_x \u001b[39m=\u001b[39m batch_x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m     batch_labels \u001b[39m=\u001b[39m batch_labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/data/Painter_clf/Experiments/Baseline/../../utils/Dataset.py:26\u001b[0m, in \u001b[0;36mimg_dset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimg_load(img_dir)\n\u001b[1;32m     25\u001b[0m \u001b[39m#resize \u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maugmenter(img)\n\u001b[1;32m     28\u001b[0m \u001b[39m#label load \u001b[39;00m\n\u001b[1;32m     29\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:138\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    136\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy(pic) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    141\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "cfg = edict({\n",
    "    'img_size' : 256,\n",
    "    'batch_size' :16 ,\n",
    "    'train_ratio' : 0.8, \n",
    "    'num_epochs' : 30, \n",
    "    'num_fold' : 5, \n",
    "    'model_name' : 'efficientnet_b4',\n",
    "    'lr' : 1e-4, \n",
    "    'device' : 'cuda:0',\n",
    "    'save_path' : './',\n",
    "    'seed' : 41 ,\n",
    "    'crop_ratio' : 0.5 \n",
    "})\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True   \n",
    "\n",
    "def score_function(real, pred):\n",
    "    score = f1_score(real, pred, average=\"macro\")\n",
    "    return score\n",
    "    \n",
    "\n",
    "def Save_record(record):\n",
    "    save_record = pd.DataFrame(record)\n",
    "    save_record.columns = ['fold','epoch_loss','acc','epoch_val_loss','val_acc']\n",
    "    save_record.to_csv(f'{cfg.save_path}/record.csv',index=False)\n",
    "    \n",
    "\n",
    "seed_everything(cfg.seed) # Seed 고정\n",
    "record = [] \n",
    "img_dirs,labels = Data_init('../../Data')\n",
    "for k,(train_index,valid_index) in enumerate(KFold(n_splits=5).split(img_dirs)):        \n",
    "    #데이터 로드 \n",
    "    if k == 0:\n",
    "        label_encoder = {value:key for key,value in enumerate(np.unique(labels))}\n",
    "    else:\n",
    "        img_dirs, labels = Data_init('./Data')\n",
    "    labels = pd.Series(labels).apply(lambda x : label_encoder[x]).values\n",
    "\n",
    "    #Train - Valid split \n",
    "    train_img_dirs, valid_img_dirs = img_dirs[train_index],img_dirs[valid_index]\n",
    "    train_labels, valid_labels = labels[train_index],labels[valid_index]\n",
    "    \n",
    "    #데이터셋, 데이터 로더 \n",
    "    train_loader = Data_loader(train_img_dirs,train_labels,cfg,augmenter=dacon_augmenter(cfg))\n",
    "    valid_loader = Data_loader(valid_img_dirs,valid_labels,cfg,augmenter=valid_augmenter(cfg),shuffle=False)\n",
    "    \n",
    "    #모델 config \n",
    "    model = Model(cfg.model_name,num_classes=len(np.unique(labels))).to(cfg.device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(cfg.device)\n",
    "    optimizer = torch.optim.RAdam(model.parameters(),lr=cfg.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=100,eta_min=0)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    print(f'Current Fold : {k}')\n",
    "    best = 0\n",
    "    for epoch in tqdm(range(cfg.num_epochs)):\n",
    "        epoch_loss,acc = epoch_run(train_loader,model,cfg.device,optimizer,criterion,scheduler,scaler)\n",
    "        epoch_loss_valid, acc_valid = valid_epoch_run(valid_loader,model,cfg.device,criterion)\n",
    "        print('\\nEpoch ', epoch)\n",
    "        print(f'Train Loss : {epoch_loss} | Train F1 : {acc}')\n",
    "        print(f'Valid Loss : {epoch_loss_valid} | Valid F1 : {acc_valid}')\n",
    "        record.append([k,epoch_loss,acc,epoch_loss_valid,acc_valid])\n",
    "        if epoch == 0:\n",
    "            best = acc_valid \n",
    "            if os.path.exists(cfg.save_path) == False:\n",
    "                os.mkdir(cfg.save_path)\n",
    "            else:\n",
    "                torch.save(model,cfg.save_path + f'{k}_fold_best.pt')\n",
    "        else:\n",
    "            if acc_valid > best:\n",
    "                torch.save(model,cfg.save_path + f'{k}_fold_best.pt')\n",
    "                print(f'Best_save{epoch}')\n",
    "\n",
    "    save_record = pd.DataFrame(record)\n",
    "    save_record.columns = ['fold','epoch_loss','Acc','Valid_epoch_loss','Valid_acc']\n",
    "    save_record.to_csv(f'{cfg.save_path}record.csv')\n",
    "\n",
    "df = pd.DataFrame(cfg.values()).T\n",
    "df.columns = cfg.keys() \n",
    "df.to_csv(f'{cfg.save_path}cfg.csv')\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
